{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.13.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/anjl@learn/project_langgraph_agent/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from typing import Annotated \n",
    "\n",
    "from typing_extentions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END \n",
    "from langgraph.graph.message import add_messages \n",
    "\n",
    "import os \n",
    "from dotenv import load_dotenv \n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "# from langchain.chat_models import init_chat_model\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "from langgraph.prebuilt import ToolNode \n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "from langraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    \"\"\"\n",
    "    Messages have the type \"list\". The \"add_messages\" function\n",
    "    in the annotation defines how this statekey should be updated\n",
    "    (in this case, It appends messages to the list, rather than overwriting them)\n",
    "    \"\"\"\n",
    "    messages:Annotated[list,add_messages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model = \"llama-3.1-8b-instant\")\n",
    "llm = init_chat_model(\"groq:llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## node functionality \n",
    "# def chatbot(state:State):\n",
    "#     return {\"messages\":[llm.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_builder = StateGraph(State)\n",
    "\n",
    "# ## add node\n",
    "# graph_builder.add_node(\"llmchatbot\",chatbot)\n",
    "\n",
    "# ## add edges\n",
    "# graph_builder.add_edge(START,\"llmchatbot\")\n",
    "# graph_builder.add_edge(\"llmchatbot\",END)\n",
    "\n",
    "# ## compile the graph\n",
    "# graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Visualize the graph\n",
    "\n",
    "# try:\n",
    "#     display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "# except Exception:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = graph.invoke({\"messages\":\"Hello\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for event in graph.stream({\"messages\":\"Hello How are you?\"}):\n",
    "#     for value in event.values():\n",
    "#         print(value[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add tools to agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_internet = TavilySearch(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b\n",
    "    Args:\n",
    "    a(int) : first int\n",
    "    b(int) : second int\n",
    "\n",
    "    Returns:\n",
    "    int : product of a and b\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [search_internet, multiply]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bind llm with tools\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize memory \n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_calling_llm(state:State):\n",
    "    return {\"messages\":[llm_with_tools.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "builder.add_node(\"tool_calling_llm\",tool_calling_llm)\n",
    "builder.add_node(\"tools\",ToolNode(tools))\n",
    "\n",
    "builder.add_edge(START,\"tool_calling_llm\")\n",
    "builder.add_conditional_edges(\n",
    "    \"tool_calling_llm\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is not a tool call -> tools_condition routes to END\n",
    "    tools_condition\n",
    ")\n",
    "# builder.add_edge(\"tools\",END)\n",
    "builder.add_edge(\"tools\",\"tool_calling_llm\")\n",
    "\n",
    "## compile the graph\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"messages\":\"What is the recent ai news\"})\n",
    "graph.invoke({\"messages\":\"What is 2 multiplied by 3\"})\n",
    "graph.invoke({\"messages\":\"What is 2 multiplied by 3 and then multiplied by 6\"})\n",
    "graph.invoke({\"messages\":\"Give me the recent ai news and then multiply 3 by 6\"})\n",
    "\n",
    "\n",
    "## use memory\n",
    "config = {\"configurable\":{\"thread_id\":\"1\"}}\n",
    "\n",
    "graph.invoke({\"messages\":\"Hi my name is Anjali\"},config=config)\n",
    "graph.invoke({\"messages\":\"What is my name?\"},config=config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
